{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk.sentiment.vader as vd\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API keys from the login credential file\n",
    "log_cred = pd.read_csv(\"login_credentials.csv\")\n",
    "consumer_key = log_cred.iloc[0, 1]\n",
    "consumer_secret = log_cred.iloc[1, 1]\n",
    "access_token = log_cred.iloc[2, 1]\n",
    "access_token_secret = log_cred.iloc[3, 1]\n",
    "#print(consumer_key, consumer_secret, access_token, access_token_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the authentication object and the API object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_by_search_term(search_term, num_tweets):\n",
    "    data = []\n",
    "    counter = 0\n",
    "    query_term = f\"{search_term} -filter:retweets\"\n",
    "    for tweet in tweepy.Cursor(\n",
    "        api.search_tweets,\n",
    "        q=query_term,\n",
    "        count=num_tweets,\n",
    "        lang=\"en\",\n",
    "        tweet_mode=\"extended\",\n",
    "    ).items():\n",
    "        tweet_details = {}\n",
    "        tweet_details[\"UserId\"] = tweet.user.name\n",
    "        tweet_details[\"TweetId\"] = tweet.id\n",
    "        tweet_details[\"tweet\"] = tweet.full_text\n",
    "        tweet_details[\"location\"] = tweet.user.location\n",
    "        tweet_details[\"created\"] = tweet.created_at.strftime(\"%d-%b-%Y\")\n",
    "        data.append(tweet_details)\n",
    "        counter += 1\n",
    "        if counter == num_tweets:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    data_df = pd.DataFrame(data)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term =  input(\"Enter the search term or hashtag here: \") # Use AND or OR to search for multiple terms\n",
    "no_of_tweets = int(input(\"Enter the number of tweets to be searched: \"))\n",
    "data_df = get_tweets_by_search_term(search_term, no_of_tweets)\n",
    "data_df = data_df.dropna()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary exploratory analysis\n",
    "print('Dataset shape:', data_df.shape)\n",
    "print('Dataset columns:', data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After droping duplicates\n",
    "print('Dataset shape:', data_df.shape)\n",
    "print('Dataset columns:', data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "data_df.drop_duplicates(subset='tweet', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tweets and store them in a dataframe\n",
    "# Drop all columns except for Tweet and create a list of all words\n",
    "tweet_df = data_df.drop(['UserId', 'TweetId', 'location', 'created'], axis=1)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cleaning\n",
    "def clean_tweet(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # Remove @mentions\n",
    "    text = re.sub(r'#', '', text) # Remove the # in #hashtag\n",
    "    text = re.sub(r'RT[\\s]+', '', text) # Remove RT (retweet)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Remove hyperlinks\n",
    "\n",
    "    return text\n",
    "\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(clean_tweet)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(word_tokenize)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "additional = ['rt', 'rts', 'r']\n",
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words('english') + additional)\n",
    "    return [word for word in tweet if word not in stop_words]\n",
    "\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(remove_stopwords)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the tweets\n",
    "def lemmatize(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(lemmatize)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final words after removing stopwords, links and lemmatization\n",
    "def final_text(words):\n",
    "    return ' '.join(words)\n",
    "\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(final_text)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TextBlob to get the sentiment of the tweets\n",
    "def get_subjectivity(tweet):\n",
    "    return TextBlob(tweet).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud of the top 100 most used words\n",
    "all_words = ' '.join([text for text in tweet_df['tweet']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 16))\n",
    "word_cloud = WordCloud(max_font_size=50, max_words=50, random_state=21).generate(all_words)\n",
    "\n",
    "ax.imshow(word_cloud, interpolation='bilinear')\n",
    "ax.set_title(f'WordCloud of the 20 Most Used Words in Recent Tweets - {search_term}', fontsize=12)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity of the tweets using TextBlob\n",
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "tweet_df['subjectivity'] = tweet_df['tweet'].apply(get_subjectivity)\n",
    "tweet_df['polarity'] = tweet_df['tweet'].apply(get_polarity)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute negative, neutral, positive analysis of the tweets\n",
    "def get_analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score== 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "tweet_df['analysis'] = tweet_df['polarity'].apply(get_analysis)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_subjectivity(score):\n",
    "    if score < 0.5:\n",
    "        return 'Objective'\n",
    "    else:\n",
    "        return 'Subjective'\n",
    "\n",
    "tweet_df['subjectivity_analysis'] = tweet_df['subjectivity'].apply(analyse_subjectivity)\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of positive, negative and neutral tweets\n",
    "tweet_df['analysis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the polarity of the tweets\n",
    "ax0 = tweet_df['polarity'].hist(bins=20, color='blue', edgecolor='black', linewidth=1.2, figsize=(10, 6))\n",
    "ax0.set_title(f'Histogram of the Polarity of the Tweets - {search_term}', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the polarity of the tweets\n",
    "ax1 = tweet_df['analysis'].value_counts().plot(kind='bar', color=['green', 'blue', 'red'], figsize=(10, 8))\n",
    "ax1.set_title(f'Analysis of Tweets by Search Term: {search_term}', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of the polarity of the tweets\n",
    "ax3 = tweet_df['analysis'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(10, 8))\n",
    "ax3.set_title(f'% of Positive, Negative and Neutral Tweets - {search_term}', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of the subjectivity of the tweets\n",
    "ax3 = tweet_df['subjectivity_analysis'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(10, 8))\n",
    "ax3.set_title(f'% of Subjective Tweets - {search_term}', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot polarity and subjectivity of the tweets\n",
    "ax4 = tweet_df.plot(kind='scatter', x='polarity', y='subjectivity', color='blue', figsize=(16,8))\n",
    "ax4.title.set_text(f'Sentiment Analysis of 1000 Tweets Mentioning - {search_term}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a009fb6d19a98b6d3101c608b2a824863ec0c989ecc73547ca15fea459f0d629"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
